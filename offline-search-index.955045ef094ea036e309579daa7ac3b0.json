[{"body":"Create a kubernetes cluster You can create a kubernetes cluster through the wizard-style page, and install the required plugins such as CNI and CSI. You can also save the cluster template in advance, and create a cluster quickly after selecting the template.\nPrepare to create a cluster You need to have enough available nodes. To add nodes, refer to \"Add Nodes\".\nPrepare the image or binary files of K8S, CRI, calico, CSI and other plug-ins that need to be installed. You can choose online/offline according to the network environment of the platform, and then choose the recommended K8S version on page. You can also upload the image required for deployment to your own image repository in advance, and specify the image repository during deployment. For more installation configuration, refer to \"Cluster Configuration Guide\".\nCreate a single-node experimental cluster Click \"Cluster Management\" \u003e \"Cluster\" to enter the cluster list page, and click the \"Create Cluster\" button in the upper left corner.\nEnter the \"Node Configuration\" page of the Create Cluster Wizard page. Fill in the \"Cluster Name\", such as \"test\", without selecting \"Cluster Template\". Select an available node, add it as a control node, and remove the taint from the master node in the taint management list. Click the \"Next\" button.\nEnter the \"Cluster Configuration\" page of the Create Cluster Wizard page. Select \"Offline Installation\", no need to specify \"Mirror Repository\", retain the default values for other configurations, click the \"Quick Create\" button, jump to the configuration confirmation page, and click the \"OK\" button.\nThe experimental cluster of a single node is created. You can view the cluster details on the cluster details page, or click the \"ViewLog\" button to view the real-time log during the cluster creation process.\nCreate a cluster using a mirror repository If you create a cluster that contains large images, it is recommended that you upload all images to a specific image repository, the creating process will be faster and smoother.\nAdd a mirror repository. Click \"Cluster Management\" \u003e \"Mirror Repository\" to enter the mirror repository list page, and click the \"Add\" button in the upper left corner. Enter the IP address of the repository where the mirror is stored in the pop-up window of adding a mirror repository, and click the \"OK\" button.\nCreate a cluster. Click \"Cluster Management\" \u003e \"Cluster\" to enter the cluster list page, and click the \"Create Cluster\" button in the upper left corner. Configure the cluster nodes as needed. In the \"Mirror Repository\" of the \"Cluster Configuration\" page, select the mirror repository added in the first step, and create the cluster after completing other configurations of the cluster as needed.\nCreate a cluster using the cluster template You can use cluster templates to simplify the cluster creation process.\nAdd a template. There are two ways to save a template. You can add a cluster template on the \"Cluster Management\" \u003e \"Template Management\" page, and select the template when creating a new cluster. You can also save the existing cluster configuration as a template by clicking \"More\" \u003e \"Save as Template\" in the cluster operation, so as to create a K8S cluster with the same configuration as the former cluster.\nCreate a cluster. Click \"Cluster Management\" \u003e \"Cluster\" to enter the cluster list page, click the \"Create Cluster\" button in the upper left corner, enter the cluster creation page, fill in the \"cluster name\", such as \"demo\", select the cluster template saved in the first step, Add the required nodes, click the \"Quick Create\" button in the lower right corner, jump to the \"Configuration Confirmation\" page, after checking the template information, click the \"OK\" button to create a cluster.\nCluster Configuration Guide Node configuration steps On the node configuration page, you can configure the node as follows:\nRegion: The region to which the cluster belongs. When adding a node, a physical or logical region can be specified for the node. The K8S cluster created by the node under this area also belongs to this region. Creating a cluster using multiple regional nodes is not supported.\nControl Nodes: Specify an odd number of control nodes for the cluster. The production environments generally use 3 control nodes to achieve high availability.\nWorker nodes: Add worker nodes to the new cluster according to the business size.\nTaint management: You can configure taint for added nodes, kubeclipper will automatically add no schedule taint to the control nodes, and you can also make changes as needed.\nNode Labels: You can configure labels for added cluster nodes as needed.\nYou can configure the required nodes according to your business needs. If you need to create a non-highly available experimental cluster, you can also add only one control node, and remove the taint automatically added for the control node. For details, see \"Creating a Single-Node Experimental Cluster\".\nCluster configuration steps On the cluster configuration page, you can configure the cluster as follows:\nInstallation method and mirror repository:\nPage configuration Configure Package/Image Sources Online (public network environment)Mirror repository is empty Configuration package source: Download from kubeclipper.io.Image pull method: The image is pulled from the official image repository by default, such as k8s image pulled from k8s.gcr.io, calico pulled from docker.io. Online (public network environment)Mirror repository specification Configuration package source: Download from kubeclipper.io.Image pull method: Pull from the filled mirror repository. The components will inherit the repository address by default. Please ensure that the repository has the related component images. You can also set an independent mirror repository for a specific component, and the component image will be pulled from this address. Offline (intranet environment)Mirror repository is empty Configuration package source: Download from the local kubeclipper cluster server node, you can use the “kcctl resource list” command to check the available configuration packages, or use the “kcctl resource push” command to upload the required configuration packages.Image pull method: Download from the local kubeclipper cluster server node. CRI will import the image after downloading. You can use the “kcctl resource list” command to check the available image packages, or use the “kcctl resource push” command to upload the required image packages. Offline (intranet environment)Mirror repository specification Configuration package source: Download from the local kubeclipper cluster server node, you can use the “kcctl resource list” command to check the available configuration packages, or use the “kcctl resource push” command to upload the required configuration packages.Image pull method: Pull from the filled mirror repository. The components will inherit the repository address by default. Please ensure that the repository has the related component images. You can also set an independent mirror repository for a specific component, and the component image will be pulled from this address. kubeclipper provides the Docker Registry and uses the kcctl registry command for management. You can also use your own image repositories. K8S version: Specify the cluster K8S version. When you choose to install offline, you can choose from the K8S version of the configuration package in the current environment; when you choose to install online, you can choose from the officially recommended version of kubeclipper.\nETCD Data Dir: You can specify the ETCD data directory, the default is /var/lib/etcd.\nCertSANs: The IP address or domain name of the k8s cluster ca certificate signature, more than one can be filled in.\nContainer Runtime: According to the specified K8S version, the default container runtime is Docker for K8S version before v1.20.0, the default container runtime is Contianerd after v1.20.0; Docker is not supported after v1.24.0.\nContainer Runtime version: Specify the containerd/docker version. As with K8S, when you choose to install offline, you can choose from the version of the configuration package in the current environment; when you choose to install online, you can choose from the officially recommended version of kubeclipper.\nContainerd data Path: The \"root dir\" in the config.toml configuration can be filled in. The default is /var/lib/containerd.\nDocker data Path: The \"root dir\" in the daemon.json configuration can be filled in . The default is /var/lib/docker.\nContainerd image repository: The repository address where the containerd image is stored, the \"registry.mirrors\" in the config.toml configuration, more than one can be filled in.\nDocker image repository: The repository address where the Docker image is stored, the insecure registry in the daemon.json configuration, more than one can be filled in.\nDNS domain name: The domain name of the k8s cluster, the default is cluster.local.\nWorker load IP: Used for load balancing from worker nodes to multiple masters, a single master does not need to be set.\nExternal access IP: You can fill in a floating IP for user access, which can be empty.\nCNI configuration The current version kubeclipper only supports Calico as cluster CNI.\nCalico divides the pod CIDR set by users into several blocks (network segments), dynamically allocates them to the required nodes according to business requirements, and maintains the routing table of the cluster nodes through the bgp peer in the nodes.\nFor example: container address pool: 172.25.0.0/16, dynamically allocated network segment pool: 172.25.0.0 - 172.25.255.192 (172.25.0.0/26 i.e. 10 bits), the number of dynamically allocated network segments: 1023, the number of pods per network segment: 61 (193-254), the total number of pods is 1023 * 61 = 62403, the relative maximum number of nodes (according to the 200 service pod as the reference value): 312.\nClusters larger than 50 nodes are currently not recommended. Clusters larger than 50 nodes are recommended to manually configure route reflection to optimize the stability of routing table maintenance for nodes in the cluster.\nTo use Calico as the cluster CNI, you need the following configuration:\nCalico mode: 5 network modes are supported:\nOverlay-IPIP-All: Use IP-in-IP technology to open up the network of pods of different nodes. Usually, this method is used in the environment where the underlying platform is IaaS. Of course, if your underlying network environment is directly a physical device, it is also completely can be used, but the efficiency and flexibility will be greatly reduced. It should be noted that you need to confirm that the underlying network environment (underlay) supports the IPIP protocol. (The network method using overlay will have a certain impact on network performance). Overlay-Vxlan-All: Use IP-in-IP technology to open up the network of pods of different nodes. Usually, this method is used in the environment where the underlying platform is IaaS. Of course, if your underlying network environment is directly a physical device, it is also completely can be used, but the efficiency and flexibility will be greatly reduced. In theory, it can run on any network environment. Usually, we will use it when the underlying environment does not support the IPIP protocol. (The network method using overlay has a certain impact on network performance). BGP : Use IP-in-IP technology to open up the network of pods of different nodes. Usually this method is used in a bare metal environment. Of course, if the Iaas platform supports BGP, it can also be used. In this mode, the IP communication of pods is accomplished by exchanging routing tables among nodes in the cluster. If you need to manually open up the pod network between multiple clusters, you need to pay attention that the addresses you assign should not conflict. Overly-IPIP-Cross-Subnet: Use IP-in-IP technology to open up the network of pods of different nodes. Usually this method is used in the environment where the underlying platform is IaaS . It should be noted that you need to confirm the underlying network environment (underlay) supports the IPIP protocol. The difference with Overlay-IPIP-All is that if two upper Pods of different nodes in the same network segment communicate with each other through the routing table, the efficiency of upper Pods of different nodes in the same network segment can be improved. Overly-Vxlan-Cross-Subnet: The logic is similar to that of Overly-IPIP-Cross-Subnet. IP version: The IP version can be specified as IPV4 or IPV4 IPV6 dual stack.\nService subnet: Fill in the service subnet CIDR, v4 defaults to: 10.96.0.0/16, v6 defaults to fd03::/112, note that the Service network must not overlap with any host network.\nPod CIDR: Fill in the pod subnet CIDR, v4 default: 172.25.0.0/24, v6 default is fd05::/120, note that the Pod network must not overlap with any host network.\nThe bottom layer of the pod network:\nFirst-found (default): The program will traverse all valid IP addresses (local, loop back, docker bridge, etc. will be automatically excluded) according to ipfamily (v4 or v6). Usually, if it is a multi-network interface card, it will exclude the default gateway. The network interface card ip other than the gateway will be used as the routing address between nodes. Can-reach: Set the routing address between nodes by checking the reachability of the domain names or IP addresses. Interface: Get all network interface card device names that satisfy the regular expression and return the address of the first network interface card as the routing address between nodes. MTU: Configure the maximum transmission unit (MTU) for the Calico environment. It is recommended to be no larger than 1440. The default is 1440. See https://docs.projectcalico.org/networking/mtu for details.\nStorage configuration The current version of Kubeclipper supports NFS as external storage types.\nConnect to NFS storage For NFS type external storage, you need to set the following:\nField Function description description/optional ServerAddr ServerAddr, the service address of NFS Required SharedPath SharedPath, the service mount path for NFS Required StorageClassName StorageClassName, the name of the storage class The default is nfs-sc, the name can be customized, and it cannot be repeated with other storage classes in the cluster ReclaimPolicy ReclaimPolicy, VPC recovery strategy Delete/Retain ArchiveOnDelete ArchiveOnDelete, whether to archive PVC after deletion Yes/No MountOptions MountOptions, the options parameter of NFS, such as nfsvers = 4.1 Optional, you can fill in several Replicas Replicas, number of NFS provisioners Default is 1 By default, multiple NFS storage types can be connected. Click the \"Continue to add\" button below to add another NFS storage. Note that the storage class name cannot be repeated.\nAfter setting up the external storage, the card below will show the storages you have enabled. You can choose a storage class as the default storage. For PVCs that do not specify a specific StorageClass, the default storage class will be used.\nCluster operation log view On the cluster details page, click the \"operation log\" tab to view the cluster operation log list. Click the \"View Log\" button on the right side of an operation log to view the detailed logs of all steps and nodes in the pop-up window. Click the step name on the left to view the detailed log output of the execution steps.\nDuring the execution of cluster operations, click View Log, you can view real-time log updates to trace the operation execution. For tasks that fail to execute, you can also view the log to find the execution steps and nodes marked with red dots, quickly locate errors, and troubleshoot the cause of operation failure.\nAccess cluster kubectl You can access the kubectl of the running cluster, click \"More\" \u003e \"Connect Terminal\" in the cluster operation, and you can execute the kubectl command line operation in the cluster kuebectl pop-up window.\nCluster plugin management In addition to installing plugins when creating a cluster, you can also install plugins for a running cluster. Taking the installation of storage plugins as an example, click the \"More\" \u003e \"Add Storage Item\" button in the cluster operation to enter the Add Storage Item page. You can install NFS plugins for the cluster. The installation configuration is the same as the configuration in cluster creation.\nFor installed plugins, you can view the plugin information on the cluster details page. You can click the \"Save as Template\" button in the upper right corner of the plugin card to save the plugin information as a template. You can also uninstall the cluster plugin by clicking the \"Remove\" button in the upper right corner of the plugin card.\nCluster node management On the \"Nodes\" list page of the cluster detail page, you can view the list of nodes in the current cluster, their specifications, status and role information.\nAdd cluster node When the cluster load is high, you can add nodes to the cluster to expand capacity. Adding nodes does not affect the running services.\nOn the cluster details page, under the Node List tab, click the \"Add Node\" button on the left, select the available nodes in the pop-up window, set the node labels, and click the \"OK\" button. The current version only supports adding worker nodes.\nRemove cluster node On the cluster details page, under the Node List tab, you can remove a node by clicking the \"Remove\" button on the right of the node. The current version only supports removing worker nodes.\nNote: To remove cluster nodes, you need to pay attention to security issues in production to avoid application interruptions.\nCluster version upgrade If the cluster version does not meet the requirements, you can upgrade the K8S version for the cluster. Similar to creating a cluster, you need to prepare the configuration package required for the cluster version and the K8S image of the target version and upload it to the specified location. For details, see \"Preparing to Create a Cluster\".\nClick the \"More\" \u003e \"Cluster Upgrade\" button of the cluster operation. In the cluster upgrade pop-up window, select the installation mode and mirror repository, and select the target version of the upgrade. The installation method of the upgrade and the configuration of the K8S version are the same as those of creating a cluster. For details, please refer to \"Cluster Configuration Guide\".\nCluster upgrades can be performed across minor versions, but upgrades skipped over later versions are not supported. For example, you can upgrade from v1.20.2 to v1.20.13, or from v1.20.x to v1.21.x, but not from v1.20.x to v1.22.x. For version 1.23.x, upgrading to version 1.24.x is not currently supported.\nThe cluster upgrade operation may take a long time. You can view the operation log on the cluster details page to track the cluster upgrade status.\nCluster Backup and Recovery The backup of K8S cluster by KubeClipper mainly backs up ETCD database data, and k8s resource object, such as namespaces, deployments, configMaps. The files and data generated by the resource itself are not backed up. For example, the data and files generated by the mysql pod will not be backed up. Similarly, the files under the PV object of the file class are not backed up, only the pv object is backed up. The backup function provided by KubeClipper is hot backup, which does not affect cluster usage during backup. While KubeClipper is not against backing up during the \"busy period\" of the cluster, it also strongly disapproves of backing up during the \"busy period\" of the cluster.\nCreate a backup point Before performing a backup, you need to set a backup point for the cluster, that is, set the storage location of the backup files. The storage type of the backup point can be FS storage or S3 storage . The following are node local storage , NFS storage and MINIO storage as examples:\nNode local storage (only for single-node experimental clusters): Create a storage directory. Connect to the cluster master node terminal (see Connect Nodes Terminal) and use the mkdir command to create the \"/root/backup\" directory in the master node.\nCreate a backup point. Click \"Cluster Management\" \u003e \"Backup Point\" to enter the backup point list page, click the \"Create\" button in the upper left corner, in the Create Backup Point pop-up window, enter \"Backup Point Name\", such as \"local\", select \"Storage Type\" as \"FS\", fill in \"Backup Path\", such as \"/root/backup\".\nSet up a cluster backup point. When creating a cluster, select \"Backup Point\" as \"local\" on the \"Cluster Configuration\" page, or edit an existing cluster and select \"local\" in the \"Backup Point\" pop-up.\nNote: Using a local node to store backup files does not require the introduction of external storage. The disadvantage is that if the local node is damaged, the backup files will also be lost, so it is strongly disapproved in a production environment .\nNFS： Prepare NFS storage. Prepare an NFS service and create a directory on the NFS server to store backup files, such as \"/data/kubeclipper/cluster-backups\".\nMount the storage directory. Connect the cluster master node terminal (see Connect node Terminal), use the mkdir command to create the \"/data/kubeclipper/cluster-backups\" directory in each master node, and mount it to the /data/kubeclipper/cluster-backups directory of the NFS server. Command example: mount -t nfs {NFS_IP}:/data/kubeclipper/cluster-backups /opt/kubeclipper/cluster-backups -o proto = tcp -o nolock.\nCreate a backup point. Click \"Cluster Management\" \u003e \"Backup Point\" to enter the backup point list page, click the \"Create\" button in the upper left corner, in the Create Backup Point pop-up window, enter \"Backup Point Name\", such as \"nfs\", select \"Storage Type\" as \"FS\", fill in \"Backup Path\" as \"/opt/kubeclipper/cluster-backups\".\nSet up a cluster backup point. When creating a cluster, select \"Backup Point\" as \"nfs\" on the \"Cluster Configuration\" page, or edit an existing cluster and select \"nfs\" in the \"Backup Point\" pop-up.\nMINIO： Prepare MINIO storage. Build MINIO services, refer to the official website https://docs.min.io/docs/minio-quickstart-guide.html for the deployment process, or use existing MINIO services.\nCreate a backup point. Click \"Cluster Management\" \u003e \"Backup Point\" to enter the backup point list page, click the \"Create\" button in the upper left corner, in the Create Backup Point pop-up window, enter \"Backup Point Name\", such as \"minio\", select \"Storage Type\" as \"S3\", fill in \"bucket name\", such as \"kubeclipper-backups\", the bucket will be automatically created by kubeclipper, fill in the IP and port number of the MINIO storage service in the first step in \"Endpoint\", fill in the service username and password, click the \"OK\" button.\nSet up a cluster backup point. When creating a cluster, select \"backup point\" as \"minio\" on the \"Cluster Configuration\" page, or edit an existing cluster and select \"minio\" in the \"Backup Point\" pop-up.\nYou can view the list and details of all backup points on the \"Backup Points\" page of \"Cluster Management\" and do the following:\nEdit: Edit the backup point description, and the username/password of the S3 type backup point.\nDelete: Delete the backup point. If there are backup files under the backup point, deletion is not allowed.\nCluster backup You can back up your cluster ETCD data by clicking the \"More\" \u003e \"Cluster Backup\" button in the cluster operation.\nYou can view all backup files for the current cluster under the Backup tab on the cluster details page, and you can also perform the following operations for backups:\nEdit: Edit the backup description.\nRestore: Performs a cluster restore operation to restore the cluster to the specified backup state.\nDelete: Deletes the backup file.\nScheduled backup You can also create a timed backup for the cluster, click the \"More\" \u003e \"Scheduled Backup\" button in the cluster operation, in the timed backup pop-up window, enter the timed backup name, execution type ( repeat / only once) and execution time, and set the number of valid backups for repeated timed backups, and click the \"OK\" button.\nkubeClipper will perform backup tasks for the cluster at the execution time you set, and the backup file will be automatically named \"Cluster Name - Timed Backup Name - Random Code\". For repeated timed backups, when the number of backup files exceeds the number of valid backup files, kubeClipper will automatically delete the later backup files.\nAfter the scheduled backup is added, you can view the scheduled backup information on the \"Scheduled Backup\" tab of the cluster details page, and you can also view the backup files generated by the scheduled backup on the \"Backup\" tab.\nFor scheduled backup tasks, you can also perform the following operations:\nEdit: Edit the execution time of the scheduled backup task and the number of valid backups for repeated scheduled backups.\nEnable/Disable: Disabled scheduled backup tasks are temporarily stopped.\nDelete: Deletes a scheduled backup task.\nCluster Backup Restore If you perform restore operation while the cluster is running, KubeClipper will perform overlay recovery on the cluster, that is, backup the ETCD data in the file, overwriting the existing data .\nYou can click the \"Restore\" button on the right side of the backup under the Backup tab of the cluster details page; or click the \"More\" \u003e \"Restore Cluster\" button in the cluster operation, and select the backup to be restored in the Restore Cluster pop-up window. The current cluster can be restored to the specified backup state.\nNote: After the K8S version of the cluster is upgraded, it will no longer be possible to restore the backup to the pre-upgrade version.\n","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","ref":"/en/docs/tutorials/cluster-manage/","tags":"","title":"Cluster management"},{"body":"For users who are new to KubeClipper and want to get started quickly, it is recommended to use the All-in-One installation mode, which can help you quickly deploy KubeClipper with zero configuration.\nDeploy KubeClipper Download kcctl KubeClipper provides a command line tool 🔧 kcctl to simplify operation and maintenance. You can download the latest version of kcctl directly with the following command:\n# The latest distribution is installed by default curl -sfL https://oss.kubeclipper.io/kcctl.sh | bash - # Install the specified version curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_VERSION=v1.2.1 bash - #If you are in China, you can use cn environment variables during installation, in this case we will use registry.aliyuncs.com/google_containers instead of k8s.gcr.io Curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn bash - You can also download the specified version from the [GitHub Release Page] ( https://github.com/kubeclipper/kubeclipper/releases ) .\nCheck if the installation was successful with the following command:\nKcctl version Start installation In this quickstart tutorial, you only need to execute one command to install KubeClipper with a template like this:\nKcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) If you use the ssh passwd method, the command is as follows:\nKcctl deploy --user root --passwd $SSH_PASSWD The private key is as follows:\nKcctl deploy --user root --pk-file $SSH_PRIVATE_KEY You only need to provide the ssh user and ssh passwd or ssh private key to deploy KubeClipper natively.\nAfter executing this command, Kcctl will check your installation environment, and if the conditions are met, it will enter the installation process. After printing the following KubeClipper banner, the installation is complete.\n_ __ _ _____ _ _ | | / / | | / __ \\ ( _) | |/ / _ _ | |__ ___| / \\/ | _ _ _ __ _ __ ___ _ __ | \\| | | | ' _\\/_ \\ | | | | ' _\\ | '_\\/_ \\ '__| | |\\ \\ | _ | | | _ ) | __/ \\__/\\ | | | _ ) | | _ ) | __/ | \\ _ |\\ _ /\\__, _ | _ .__/ \\___|\\____/ _ | _ | .__/| .__/ \\___| _ | | | | | | _ | | _ | You can also deploy the master version of KubeClipper to experience the latest features\nInstall kcctl curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_VERSION=master bash - Set environment variables on the installation server export KC_VERSION=master Deploy KubeClipper AIO environment kcctl deploy Login to console After the installation is complete, open a browser and visit http://$IP to enter the KubeClipper console.\nYou can use the default account password \" admin/Thinkbig1 \" to log in.\nYou may need to configure port forwarding rules and open ports in security groups for external users to access the console.\nCreate k8s cluster After successful deployment you can create a k8s cluster using the ** kcctl tool ** or via the ** console ** . Use the kcctl tool to create it in this quickstart tutorial.\nFirst, use the default account password to log in and obtain the token, which is convenient for subsequent interaction between kcctl and kc-server.\nKcctl login -H http://localhost -u admin -p Thinkbig1 Then create a k8s cluster with the following command:\nNODE = $ (kcctl get node -o yaml | grep ipv4DefaultIP: | sed's/ipv4DefaultIP : //') Kcctl create cluster --master $NODE --name demo --untaint-master It takes about 3 minutes to complete the cluster creation, or you can use the following command to view the cluster status\nKcctl get cluster -o yaml | grep status -A5 You can also go to the console to view the real-time log.\nEntering the Running state means that the cluster installation is complete, you can use the kubectl get cs command to view the cluster health.\n","categories":["QuickStart"],"description":"Deploying the AIO environment\n","excerpt":"Deploying the AIO environment\n","ref":"/en/docs/getting-started/aio_env/","tags":["aio","sample","docs"],"title":"Deploying AIO"},{"body":"创建集群 您可以通过向导式的页面创建 K8S 集群，并安装CNI、CSI等所需插件。也可以提前保存好集群模版，选择模版后快速创建集群。\n创建集群准备工作 您需要准备充足的可用节点，如需添加节点，参见“添加节点”教程。\n准备好需要部署的 K8S、CRI、calico、CSI 和其他插件的镜像或二进制文件，kubeclipper 提供了推荐的版本，您可以根据平台所处网络环境，选择在线 / 离线后，直接在页面上选取使用。您也可以将部署所需的镜像上传至自己的镜像仓库，并在部署时指定。更多安装配置，参考以下表格：\n页面配置 配置包/镜像来源 在线（公网环境）\n镜像仓库为空 1. 配置包来源：从 kubeclipper.io 下载。\n2. 镜像拉取方式：镜像默认从官方镜像仓库拉取，如k8s镜像从 k8s.gcr.io 拉取、calico 从 docker.io 拉取。 在线（公网环境）\n镜像仓库指定 1. 配置包来源：从 kubeclipper.io 下载。\n2. 镜像拉取方式：从填写的镜像仓库拉取，组件将默认继承该仓库地址，请确保该仓库存在相关组件镜像；组件也会提供独立的镜像仓库参数，设置后组件镜像从该地址拉取。 离线（内网环境）\n镜像仓库为空\n1. 配置包来源：从本地 kubeclipper 集群server节点下载，您可以使用 kcctl resource list 命令查看本地可用配置包，或使用 kcctl resource push 命令上传所需配置包。\n2. 镜像拉取方式：从本地 kubeclipper 集群server节点下载，下载后由CRI进行镜像导入。您可以使用 kcctl resource list 命令查看本地可用镜像包，或使用 kcctl resource push 命令上传所需镜像包。 离线（内网环境）\n镜像仓库指定\n1. 配置包来源：从本地下载，您可以使用 kcctl resource list 命令查看本地可用配置包，或使用 kcctl resource push 命令上传所需配置包。\n2. 镜像拉取方式：从填写的镜像仓库拉取，组件将默认继承该仓库地址，请确保该仓库存在相关组件镜像；组件也会提供独立的镜像仓库参数，设置后组件镜像从该地址拉取。kubeclipper 提供 Docker Registry 方案，并使用 kcctl registry 命令行进行管理，您也可以使用其他自有镜像仓库。 创建单节点实验集群 点击“集群管理”\u003e“集群”，进入集群列表页面，点击左上角“创建集群”按钮。\n进入创建集群向导页面的“节点配置”页面。填写“集群名称”，如“test”，不需选择“集群模版”。选择一个可用节点，添加为控制节点，并在污点管理列表中，将 master 节点的污点移除。点击“下一步”按钮。\n进入创建集群向导页面的“集群配置”页面。选择“离线安装”，“镜像仓库”不需填写，其他配置都可使用默认配置，点击“快速创建”按钮，跳转配置确认页面，点击“确认”按钮。\n单节点的实验集群创建完成，您可以在集群详情页查看集群详情，也可以点击“查看日志”按钮，查看集群创建过程中的实时日志。\n使用镜像仓库创建集群 如果创建的集群中包含较大的镜像，推荐您将所有镜像上传到特定的镜像仓库，创建集群会更快速更顺畅。\n添加镜像仓库。点击“集群管理”\u003e“镜像仓库”，进入镜像仓库列表页面，点击左上角“添加”按钮。在添加镜像仓库的弹窗中输入存放有镜像的仓库 IP 地址，点击“确定”按钮。\n创建集群。点击“集群管理”\u003e“集群”，进入集群列表页面，点击左上角“创建集群”按钮。按需配置集群节点，在“集群配置”页面的“镜像仓库”中，选择第一步添加的镜像仓库，根据需要完成集群其他配置后创建集群。\n使用集群模版创建集群 您可以使用集群模版，简化集群创建流程。\n添加模版。保存模版有两种方式，您可以在“集群管理”\u003e“模版管理”页面，添加集群模版，以备创建集群时使用。也可以点击集群操作中的“更多”\u003e“保存为模版”，将已存在的集群配置保存为模版，以便创建出和该集群同等配置的 K8S 集群。\n创建集群。点击“集群管理”\u003e“集群”，进入集群列表页面，点击左上角“创建集群”按钮，进入创建集群页面，填写“集群名称”，如“demo”，选择第一步中保存的集群模版，添加所需节点，点击右下角“快速创建”按钮，跳转至“配置确认”页面，核对模版信息无误后，点击“确认”按钮，创建集群。\n集群配置指南 节点配置步骤 在节点配置页面，您可以对节点进行以下配置：\n区域：集群所属区域，添加节点时可为节点指定物理的或逻辑的区域，使用该区域下节点创建的 K8S 集群也属于该区域，不支持使用跨区域的多个节点创建集群。\n控制节点：为集群指定奇数个的控制节点，生产环境一般使用3个控制节点以实现高可用。\n工作节点：根据业务规模，为新集群添加工作节点。\n污点管理：您可以为已添加的节点配置污点，kubeclipper 会自动为控制节点添加不允许调度（noschedule）的污点，您也可以根据需要进行更改。\n节点标签：您可以根据需要为已添加的集群节点配置标签。\n您可以按业务需要配置所需节点。如果需要创建非高可用的实验集群，也可以仅添加一个控制节点，并将控制节点自动添加的污点移除，详细操作参见“创建单节点实验集群”。\n集群配置步骤 在集群配置页面，您可以对集群进行以下配置：\n安装方式和镜像仓库： 页面配置 配置包/镜像来源 在线（公网环境）\n镜像仓库为空 1. 配置包来源：从 kubeclipper.io 下载。\n2. 镜像拉取方式：镜像默认从官方镜像仓库拉取，如k8s镜像从 k8s.gcr.io 拉取、calico 从 docker.io 拉取。 在线（公网环境）\n镜像仓库指定 1. 配置包来源：从 kubeclipper.io 下载。\n2. 镜像拉取方式：从填写的镜像仓库拉取，组件将默认继承该仓库地址，请确保该仓库存在相关组件镜像；组件也会提供独立的镜像仓库参数，设置后组件镜像从该地址拉取。 离线（内网环境）\n镜像仓库为空 1. 配置包来源：从本地 kubeclipper 集群server节点下载，您可以使用 kcctl resource list 命令查看本地可用配置包，或使用 kcctl resource push 命令上传所需配置包。\n2. 镜像拉取方式：从本地 kubeclipper 集群server节点下载，下载后由CRI进行镜像导入。您可以使用 kcctl resource list 命令查看本地可用镜像包，或使用 kcctl resource push 命令上传所需镜像包。 离线（内网环境）\n镜像仓库指定\n1. 配置包来源：从本地下载，您可以使用 kcctl resource list 命令查看本地可用配置包，或使用 kcctl resource push 命令上传所需配置包。\n2. 镜像拉取方式：从填写的镜像仓库拉取，组件将默认继承该仓库地址，请确保该仓库存在相关组件镜像；组件也会提供独立的镜像仓库参数，设置后组件镜像从该地址拉取。kubeclipper 提供 Docker Registry 方案，并使用 kcctl registry 命令行进行管理，您也可以使用其他自有镜像仓库。 K8S版本：指定集群K8S版本。当您选择离线安装的时候，可以从当前环境中配置包的K8S版本中选择；当您选择在线安装的时候，可以从 kubeclipper 官方推荐的版本中选择。\nETCD数据目录：可指定ETCD 数据目录，默认为/var/lib/etcd。\nCertSANs：k8s 集群 ca 证书签名的 ip 或者域名，可填写多个。\n容器运行时：根据指定 K8S 版本，K8S 版本在 v1.20.0 之前，容器运行时默认 docker，之后默认 containerd；v1.24.0 之后不支持 docker。\n容器运行时版本：指定containerd / docker版本。与 K8S 相同，当您选择离线安装的时候，可以从当前环境中配置包的版本中选择；当您选择在线安装的时候，可以从 kubeclipper 官方推荐的版本中选择。\nContainerd 数据目录：可填写config.toml 配置中的 root dir，默认为/var/lib/containerd。\nDocker数据目录：可填写daemon.json 配置中的 root dir，默认为/var/lib/docker。\nContainerd 镜像仓库：存放containerd镜像的仓库地址，config.toml 配置中的 registry.mirrors，可填写多个。\nDocker镜像仓库：存放docker镜像的仓库地址，daemon.json 配置中的 insecure registry,可填写多个。\nDNS 域名：k8s 集群的域名，默认为cluster.local。\nWorker 负载 IP：用于 worker 节点到多master的负载均衡，单一master不需要设置。\n外部访问IP：可以填写一个浮动 IP 给用户访问，可为空。\nCNI配置 当前版本仅支持Calico作为集群CNI。\nCalico将用户设置的pod cidr分为若干个block(网段)，根据业务需求动态的分配给需要的节点，并在节点中通过 bgp peer维护集群节点的路由表。\n例如：容器的地址池：172.25.0.0/16，动态分配的网段池: 172.25.0.0 - 172.25.255.192 (172.25.0.0/26 即 10 个比特位)，动态分配的网段数: 1023，每个网段的pod数量为: 61 (193-254)，总pod数量为1023 * 61 = 62403，相对最大节点数(按照200业务pod为基准值)：312。\n目前不建议大于50个节点的集群，大于50个节点的集群建议手动配置route reflection，用来优化集群中的节点的路由表维护的稳定性。\n使用Calico作为集群CNI，您需要进行以下配置：\nCalico 模式：支持5种网络模式：\nOverlay-IPIP-All: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在底层平台是 iaas 的环境之中,当然如果你底层网路环境直接是物理设备的也完全可以使用只不过效率和灵活度都会大打折扣,需要注意的是你需要确认底层网络环境(underlay)是支持 IPIP 协议的.(使用overlay的网络方式对网络性能造成一定的影响)。\nOverlay-Vxlan-All: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在底层平台是 iaas 的环境之中,当然如果你底层网路环境直接是物理设备的也完全可以使用只不过效率和灵活度都会大打折扣,他理论上可以在任何的网络环境上运行,通常在底层环境不支持 IPIP 协议的时候我们会使用他.(使用overlay的网络方式对网络性能造成一定的影响)。\nBGP: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在裸机的环境上,当然底 Iaas 平台支持 BGP 的话也是可以使用的,这种模式下 pod 的 ip 通信是通过 集群中的各个节点中互相交换路由表来完成 pod 之间的通信的,如果你需要手动打通多个集群之间的 pod 网络需要注意你分配的地址断不应该有冲突。\nOveraly-IPIP-Cross-Subnet: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在底层平台是 iaas 的环境之中,需要注意的是你需要确认底层网络环境(underlay)是支持 IPIP 协议的.和 Overlay-IPIP-All 的不同之处在于,如果 2 个不同节点但在同一个网段中的上 pod 互相通信时是通过路由表,这样可以提高在不同节点但在同一个网段中的上 pod 互相通信时的效率。\nOveraly-Vxlan-Cross-Subnet: 和 Overaly-IPIP-Cross-Subnet 逻辑相似不再做重复的解释。\nIP版本：可指定IP版本为IPV4或IPV4 IPV6双栈。\n服务子网：填写service子网CIDR，v4默认为：10.96.0.0/16，v6默认为fd03::/112，注意Service网络不得与任何主机网络重叠。\nPod CIDR：填写pod子网CIDR，v4默认：172.25.0.0/24，v6默认为fd05::/120，注意Pod 网络不得与任何主机网络重叠。\npod网路的底层：\nfirst-found（默认）：程序会根据ipfamily(v4或v6)遍历所有的有效的ip地址(local,loop back，docker bridge等会被自动排除)通常如果是多网卡时会排除默认网关以外的网卡的ip作为节点之间的路由地址。\ncan-reach：通过检查域名或者ip的可达性来设置节点之间的路由地址。\ninterface：根据正则表达式获取所有满足的网卡设备名称并返回第一个满足表达式网卡的地址作为节点之间的路由地址。\nMTU：为Calico环境配置最大传输单元(MTU)，建议不大于1440，默认为1440，详情见 Configure MTU to maximize network performance。\n存储配置 Kubeclipper当前版本内置了 NFS、Ceph 两种外接存储类型。\n对接NFS存储 对接NFS类型的外接存储，您需要设置以下内容：\n字段 作用说明 填写说明/可选项 服务地址 ServerAddr，NFS的服务地址 必填 共享路径 SharedPath，NFS的服务挂载路径 必填 存储类 StorageClassName，存储类的名称 默认为 nfs-sc，可自定义名称，不可与集群其他存储类重复 回收策略 ReclaimPolicy，VPC回收策略 删除 Delete / 保留 Retain 删除后归档 ArchiveOnDelete，是否在删除后归档PVC 是 / 否 挂载选项 MountOptions，NFS 的 options 参数，如nfsvers=4.1 选填，可填写多个 副本数 Replicas，NFS provisioner副本数 默认为1 NFS类型的存储默认可对接多个，点击下方“继续添加”按钮，可以添加多个NFS存储，注意存储类名称不可重复。\n设置完外接存储后，下方卡片会显示您已经开启的存储，您可以选择一个存储类作为默认存储，对于未指定特定StorageClass 的 PVC ，会直接使用默认的存储类。\n集群操作日志查看 在集群详情页面，点击“操作日志”标签页，可以查看集群操作日志列表。点击操作日志右侧“查看日志”按钮，可以在弹窗中查看全部步骤和节点的详细日志。点击左侧步骤名称，可查看执行步骤详细的日志输出。\n在集群操作执行过程中，点击查看日志，您可以实时查看到日志更新来跟踪操作执行情况。对于执行失败的任务，您也可以通过查看日志，找到红色圆点标注的执行步骤和节点，快速定位错误，排查操作失败原因。\n访问集群 kubectl 您可以访问运行中集群的 kubectl，点击集群操作中的“更多”\u003e“连接终端”，就可以在集群 kuebectl 弹窗中执行 kubectl 命令行操作。\n集群插件管理 除了在创建集群时安装插件，您也可以为运行中的集群安装存储和其他自定义插件。以安装存储插件为例，点击集群操作中的“更多”\u003e“添加存储项”按钮，进入添加存储项页面，可以为集群安装 NFS 或Ceph 类型的存储插件，安装配置与创建集群中的配置相同。\n对于已安装的插件，您可以在集群详情页查看插件信息。可以点击插件卡片右上角的“保存为模版”按钮，将插件信息保存为模版，以便为其他集群使用。也可以点击插件卡片右上角的“移除”按钮，卸载集群插件。\n集群节点管理 在集群详情页的“节点”列表页面，您可以查看当前集群中的节点列表，节点的规格、状态和角色信息。\n添加集群节点 当集群负载较高时，您可以通过为集群添加节点来达到主动扩容的目的，添加新的“节点”不会影响现有的业务的运行。\n在集群详情页的节点列表标签页下，点击左侧的“添加节点”按钮，在弹窗中选择可用节点，设置节点标签，点击“确认”按钮。当前版本仅支持添加工作节点。\n移除集群节点 在集群详情页的节点列表标签页下，您可以点击节点右侧的 “移除”按钮移除节点。当前版本仅支持移除工作节点。\n注意：移除集群节点，您需要注意生产中的安全问题，避免应用发生中断。\n集群版本升级 当集群版本不满足需要，您可以为集群升级 K8S 版本。与创建集群一样，您需要准备好集群版本所需配置包和目标版本的 K8S 镜像并上传至指定位置，详情参见“创建集群准备工作”。\n点击集群操作的“更多”\u003e“集群升级”按钮，在集群升级弹窗中选择安装方式和镜像仓库，选择升级的目标版本，升级的安装方式和 K8S 版本的配置与创建集群相同，详情参见“集群配置指南”。\n集群升级可以跨小版本，但不支持略过次版本的升级，如您可以从v1.20.2升级到v1.20.13，或由v1.20.x升级到v1.21.x，但不支持从v1.20.x升级到v1.22.x。对于1.23.x版本，暂不支持升级到1.24.x版本。\n升级集群操作可能需要较长时间，您可以在集群详情页面查看操作日志，跟踪集群升级状态。\n集群备份与恢复 KubeClipper 对 k8s 集群的备份主要为备份 ETCD 数据库数据，以及 k8s 的资源对象备份，如 namespace，deployment、configMap。对资源自身产生的文件和数据不做备份，例如对集群中运行的 mysql pod，该 mysql pod 产生的数据和文件，不会为之备份，同理，文件类的 pv 对象下的文件，也不做备份，仅仅备份 pv 这个对象。KubeClipper 提供的备份功能是热备份，备份期间不影响集群的使用。KubeClipper 虽然不反对在集群 “繁忙期” 备份，但也强烈不赞成在集群 “繁忙期” 备份。\n创建备份点 执行备份之前，您需要先为集群设置备份点，即设置备份文件的存储位置。备份点的存储类型可以是FS 存储或 S3 存储，下面以节点本地存储、NFS 存储和 MINIO 存储为例：\n节点本地存储**（仅适用单节点实验集群）：**\n创建存储目录。连接集群 master 节点终端（可参见连接节点终端），使用 mkdir 命令，在 master 节点中创建“/root/backup”目录。\n创建备份点。点击“集群管理”\u003e“备份点”，进入备份点列表页，点击右上角“创建”按钮，在创建备份点弹窗中，输入“备份点名称”，如“local”，选择“存储类型”为“FS”，填写“备份路径”，如“/root/backup”。\n设置集群备份点。创建集群时，在“集群配置”页面选择“备份点”为“local”，或者编辑已有集群，在编辑集群弹窗中的“备份点”中选择“local”。\n注意：使用本地节点存储备份文件，不需要引入外部存储，缺点是如果本地节点遭到破坏，备份文件也会丢失，所以强烈不赞成在生产环境中使用。\nNFS：\n准备 NFS 存储。准备一台 NFS 服务，并在 NFS 服务器上创建一个用于存放备份文件的目录，如 “/data/kubeclipper/cluster-backups”。\n挂载存储目录。连接集群 master 节点终端（可参见连接节点终端），使用 mkdir 命令，在每个 master 节点中创建“/data/kubeclipper/cluster-backups”目录，并 mount 到 NFS 服务器的 /data/kubeclipper/cluster-backups 目录即可，命令示例：mount -t nfs { NFS_IP }:/data/kubeclipper/cluster-backups /opt/kubeclipper/cluster-backups -o proto=tcp -o nolock。\n创建备份点。点击“集群管理”\u003e“备份点”，进入备份点列表页，点击右上角“创建”按钮，在创建备份点弹窗中，输入“备份点名称”，如“nfs”，选择“存储类型”为“FS”，填写“备份路径”为“/opt/kubeclipper/cluster-backups”。\n设置集群备份点。创建集群时，在“集群配置”页面选择“备份点”为“nfs”，或者编辑已有集群，在编辑集群弹窗中的“备份点”中选择“nfs”。\nMINIO：\n准备 MINIO 存储。搭建 MINIO 服务，部署过程参考官网 https://docs.min.io/docs/minio-quickstart-guide.html，也可以使用已有 MINIO 服务。\n创建备份点。点击“集群管理”\u003e“备份点”，进入备份点列表页，点击右上角“创建”按钮，在创建备份点弹窗中，输入“备份点名称”，如“minio”，选择“存储类型”为“S3”，填写“bucket 名称”，如 “kubeclipper-backups”，该 bucket 将由 kubeclipper 自动创建，“Endpoint”中填写第一步 MINIO 存储服务的 ip 和端口号，填写服务用户名和密码，点击“确定”按钮。\n设置集群备份点。创建集群时，在“集群配置”页面选择“备份点”为“minio”，或者编辑已有集群，在编辑集群弹窗中的“备份点”中选择“minio”。\n您可以在“集群管理”的“备份点”页面查看所有备份点列表和详细信息，并执行以下操作：\n编辑：编辑备份点描述，和 S3 类型备份点的用户名/密码。\n删除：删除备份点，备份点下存在备份文件的，不允许删除。\n集群备份 您可以点击集群操作中的“更多”\u003e“集群备份”按钮，备份集群 ETCD 数据。\n您可以在集群详情页面的备份标签页下，查看当前集群的所有备份文件，还可以对备份执行以下操作：\n编辑：编辑备份描述。\n恢复：执行集群恢复操作，将集群恢复至指定备份状态。\n删除：删除指定备份文件。\n定时备份 您也可以为集群创建定时备份，点击集群操作中的“更多”\u003e“定时备份”按钮，在定时备份弹窗中，输入定时备份名称、执行类型（重复执行/仅执行一次）和执行时间，并为重复执行的定时备份设置有效备份个数，点击“确认”按钮。\nkubeClipper 会在您设置的执行时间为集群执行备份任务，备份文件会自动命名为“集群名称-定时备份名称-随机码”，对于重复执行的定时备份，kubeClipper 会在该定时任务下的备份文件超过有效备份个数时，自动删除超出个数的较晚的备份文件。\n定时备份添加完成后，可以在集群详情页的“定时备份”标签页查看定时备份信息，也可以在“备份”标签页查看定时备份产生的备份文件。\n对于定时备份任务，您还可以执行以下操作：\n编辑：编辑定时备份任务执行时间，和重复执行的定时备份的有效备份个数。\n启用/禁用：禁用的定时备份任务将暂时停止执行。\n删除：删除定时备份任务。\n集群备份恢复 如果您在集群正常运行期间执行恢复操作，则 KubeClipper 将对该集群进行覆盖式恢复，就是备份文件里面的 etcd 数据，覆盖现有的数据。\n您可以在集群详情页的备份标签页下，点击备份右侧的 “恢复”按钮；或点击集群操作中的“更多”\u003e“恢复集群”按钮，在恢复集群弹窗中选择需要恢复的备份，可以将当前集群恢复至指定备份状态。\n注意：集群升级后，将无法再恢复到升级前版本的备份。\n","categories":"","description":"KubeClipper 集群管理功能使用指南\n","excerpt":"KubeClipper 集群管理功能使用指南\n","ref":"/docs/tutorials/cluster-manage/","tags":"","title":"集群管理"},{"body":"对于初次接触 KubeClipper 并想快速上手的用户，建议使用 All-in-One 安装模式，它能够帮助您零配置快速部署 KubeClipper。\n准备工作 KubeClipper 本身并不会占用太多资源，但是为了后续更好的运行 Kubernetes 建议硬件配置不低于最低要求。\n您仅需参考以下对机器硬件和操作系统的要求准备一台主机。\n硬件推荐配置 确保您的机器满足最低硬件要求：CPU \u003e= 2 核，内存 \u003e= 2GB。 操作系统：CentOS 7.x / Ubuntu 18.04 / Ubuntu 20.04。 节点要求 节点必须能够通过 SSH 连接。 节点上可以使用 sudo / curl / wget / tar 命令。 建议您的操作系统处于干净状态（不安装任何其他软件），否则可能会发生冲突。\n部署 KubeClipper 下载 kcctl KubeClipper 提供了命令行工具🔧 kcctl 以简化运维工作，您可以直接使用以下命令下载最新版 kcctl：\n# curl -sfL https://oss.kubeclipper.io/kcctl.sh | bash - # 如果你在中国， 你可以在安装时使用 cn 环境变量, 此时我们会使用 registry.aliyuncs.com/google_containers 代替 k8s.gcr.io curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn bash - 您也可以在 GitHub Release Page 下载指定版本。\n通过以下命令检测是否安装成功:\nkcctl version 开始安装 在本快速入门教程中，您只需执行一个命令即可安装 KubeClipper，其模板如下所示：\nkcctl deploy 若使用 ssh passwd 方式则命令如下所示:\nkcctl deploy --user root --passwd $SSH_PASSWD 私钥方式如下：\nkcctl deploy --user root --pk-file $SSH_PRIVATE_KEY 您只需要提供 ssh user 以及 ssh passwd 或者 ssh 私钥即可在本机部署 KubeClipper。\n执行该命令后，Kcctl 将检查您的安装环境，若满足条件将会进入安装流程。在打印出如下的 KubeClipper banner 后即表示安装完成。\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| 登录控制台 安装完成后，打开浏览器，访问 http://$IP 即可进入 KubeClipper 控制台。\n您可以使用默认帐号密码 admin / Thinkbig1 进行登录。\n您可能需要配置端口转发规则并在安全组中开放端口，以便外部用户访问控制台。\n创建 k8s 集群 部署成功后您可以使用 kcctl 工具或者通过控制台创建 k8s 集群。在本快速入门教程中使用 kcctl 工具进行创建。\n首先使用默认帐号密码进行登录获取 token，便于后续 kcctl 和 kc-server 进行交互。\nkcctl login -H http://localhost -u admin -p Thinkbig1 然后使用以下命令创建 k8s 集群:\nNODE=$(kcctl get node -o yaml|grep ipv4DefaultIP:|sed 's/ipv4DefaultIP: //') kcctl create cluster --master $NODE --name demo --untaint-master 大概 3 分钟左右即可完成集群创建,也可以使用以下命令查看集群状态\nkcctl get cluster -o yaml|grep status -A5 您也可以进入控制台查看实时日志。\n进入 Running 状态即表示集群安装完成,您可以使用 kubectl get cs 命令来查看集群健康状况。\n","categories":["QuickStart"],"description":"快速搭建体验平台功能\n","excerpt":"快速搭建体验平台功能\n","ref":"/docs/getting-started/","tags":["docs"],"title":"快速开始"},{"body":"What is KubeClipper? KubeClipper aims to provide easy-to-use, easy-to-operate, lightweight, product-grade kubernetes multi-cluster full lifecycle management service, freeing operation and maintenance engineers from complicated configuration and obscure command lines to achieve one-stop management of multi-K8 s clusters across regions and infrastructure.\nWhy do I want KubeClipper? In the cloud-native era, Kubernetes has undoubtedly become the de facto standard for container orchestration. Although there are many tools to assist in the installation and management of K8S clusters, it is still very complicated to build and operate a production-level K8S cluster. In the process of a large number of services and practices, 99cloud has precipitated an extremely lightweight and easy-to-use graphical interface Kubernetes multi-cluster management tool - KubeClipper.\nUnder the premise of being fully compatible with native Kubernetes, KubeClipper is repackaged based on the kubeadm tool widely used by the community, providing rapid deployment of K8S clusters and continuous full life cycle management (installation, uninstallation, upgrade, scaling) in the enterprise’s own infrastructure. It supports multiple deployment methods such as online, proxy, and offline, and also provides rich and scalable management services for CRI, CNI, CSI, and various CRD components.\nCompared with the existing K8S lifecycle management tools such as Sealos, KubeKey, Kubeasz, KubeOperator, and K0S, KubeClipper is more open and native, lightweight, convenient, stable and easy to use.\nGetting Started: Get started with $project Examples: Check out some example code! ","categories":"","description":"Manage kubernetes in the most light and convenient way ☸️\n","excerpt":"Manage kubernetes in the most light and convenient way ☸️\n","ref":"/en/docs/overview/","tags":"","title":"Overview"},{"body":"KubeClipper 是什么？ KubeClipper 旨在提供易使用、易运维、极轻量、生产级的 Kubernetes 多集群全生命周期管理服务，让运维工程师从繁复的配置和晦涩的命令行中解放出来，实现一站式管理跨区域、跨基础设施的多 K8S 集群。\n为什么需要 KubeClipper？ 云原生时代，Kubernetes 已毋庸置疑地成为容器编排的事实标准。虽然有诸多辅助 K8S 集群安装和管理的工具，但搭建和运维一套生产级别的 K8S 集群仍然十分复杂。九州云在大量的服务和实践过程中，沉淀出一个极轻量、易使用的图形化界面 Kubernetes 多集群管理工具——KubeClipper。\nKubeClipper 在完全兼容原生 Kubernetes 的前提下，基于社区广泛使用的 kubeadm 工具进行二次封装，提供在企业自有基础设施中快速部署 K8S 集群和持续化全生命周期管理（安装、卸载、升级、扩缩容、远程访问等）能力，支持在线、代理、离线等多种部署方式，还提供了丰富可扩展的 CRI、CNI、CSI、以及各类 CRD 组件的管理服务。\n与现有的 Sealos、KubeKey、Kubeasz、KubeOperator、K0S 等 K8S 生命周期管理工具相比，KubeClipper 更贴近开放原生、轻量便捷、稳定易用。\n快速开始: Learn how to get started with Kubeclipper 用户手册: Check out some example code! ","categories":"","description":"Manage kubernetes in the most light and convenient way ☸️\n","excerpt":"Manage kubernetes in the most light and convenient way ☸️\n","ref":"/docs/overview/","tags":"","title":"概述"},{"body":"The purpose of this document is to deploy an HA KubeClipper with a simple operation.\nIf you just want a simple experience, please refer to QuickStart to deploy AIO environment.\nPreparations You only need to prepare a host with reference to the following requirements for machine hardware and operating system: Preparations\nHA Deploy Recommend:\nKubeclipper uses etcd as backend storage. In order to ensure high availability, it is recommended to use 3 nodes and above for deployment. At the same time, the production environment recommends that the server node and the agent node be separated to avoid acting as both the server node and the agent node at the same time. Node planning: Deploying KubeClipper Download kcctl KubeClipper provides command line tools 🔧 kcctl to simplify operation and maintenance work. You can directly download the latest version of kcctl with the following command:\ncurl -sfL https://oss.kubeclipper.io/kcctl.sh | sh - # In China, you can add cn env, we use registry.aliyuncs.com/google_containers instead of k8s.gcr.io # curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn sh - You can also download the specified version on the GitHub Release Page.\nCheck if the installation is successful with the following command:\nkcctl version Get Started with Installation All you need to do is execute a command to install KubeClipper, whose template looks like this:\nkcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) (--server SERVER_NODES) (--agent AGENT_NODES) If you use the ssh passwd method, the command is as follows:\nkcctl deploy --user root --passwd $SSH_PASSWD --server SERVER_NODES --agent AGENT_NODES The private key method is as follows:\nkcctl deploy --user root --pk-file $SSH_PRIVATE_KEY --server SERVER_NODES --agent AGENT_NODES You only need to provide ssh user and ssh passwd or ssh private key to deploy KubeClipper on the corresponding node.\nThis tutorial uses the private key to deploy, the specific commands are as follows:\nkcctl deploy --server 192.168.10.110,192.168.10.111,192.168.10.112 --agent 192.168.10.113,192.168.10.114,192.168.10.115 --pk-file ~/.ssh/id_rsa --pkg https://oss.kubeclipper.io/release/v1.1.0/kc-amd64.tar.gz This command shows kubeclipper platform will has 3 server node and 3 agent node.\nYou can visit the GitHub Release Page to view the current KubeClipper release version and modify the version number in the pkg parameter.\nFor example, after the v1.2.0 release you can specify –pkg as the https://oss.kubeclipper.io/release/v1.2.0/kc-amd64.tar.gz to install the v1.2.0 version.\nAfter you runn this command, kcctl will check your installation environment and enter the installation process, if the conditions are met. After printing the KubeClipper banner, the installation is complete.\n| | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| Login console When deployed successfully, you can open a browser and visit http://$IP to enter the KubeClipper console.\nYou can log in with the default account password admin/Thinkbig1.\nYou may need to configure port forwarding rules and open ports in security groups for external users to access the console.\n","categories":"","description":"deploy a high available kubeclipper by some simple cmd.\n","excerpt":"deploy a high available kubeclipper by some simple cmd.\n","ref":"/en/docs/deployment-docs/ha-env/","tags":"","title":"Deploy HA"},{"body":"","categories":["Examples","Placeholders"],"description":"Quickly build the experience platform function\n","excerpt":"Quickly build the experience platform function\n","ref":"/en/docs/getting-started/","tags":["test","docs"],"title":"Getting Started"},{"body":"On the \"Node Information\" page, you can view the list of all nodes managed in the platform, node specifications, status and other information. Click the node name to enter the node details page, you can view detailed node basic information and system information.\nThe node status in KubeClipper represents the management status of the node by kc-gent. Under normal circumstances, the node status is displayed as \"Ready\". When the node is out of contact for 4 minutes (within 10s of the error time), the status will be updated to \"Unknown\". Nodes with unknown status cannot perform any operations, nor can they create clusters or add/remove nodes for clusters.\nAdd node When deploying KubeClipper, you can add the initial server nodes which are used to deploy KubeClipper's own services, and agent nodes which are used to deploy K8S clusters. In a KubeClipper environment for experimentation or development, you can add a server node as an agent node at the same time. However, if it is used in a production environment, it is recommended not to reuse the server node as an agent node.\nYou can also use the kcctl join command to add agent nodes to KubeClipper, and mark a region for each agent node. The region can be a physical or logical location. You can use nodes in the same region to create a K8S cluster, but you cannot use nodes across regions to create a cluster. Nodes in unmarked regions belong to the default region. For details, see \"Kcctl Operation Guide\".\nCommand line example:\nkcctl join --agent beijing:1.2.3.4 --agent shanghai:2.3.4.5 Remove node When you no longer need some nodes, you can use the kcctl drain command to remove nodes from the platform. See \"Kcctl Operation Guide\" for details.\nCommand line example:\nkcctl drain --agent 192.168.10.19 Connect Terminal On the node list page, you can click the \"Connect Terminal\" button on the right side of the target node, enter the node port and username password information in the pop-up window, access the node SSH console and execute the command.\n","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","ref":"/en/docs/tutorials/node-manage/","tags":"","title":"Node \u0026 Zone Management"},{"body":"本文档旨在通过简单的操作部署一个 HA 版本的 kubeclipper。\n如果只是想简单体验一下，请参考 QuickStart 部署 AIO 环境。\n准备工作 HA 部署注意事项：\nkubeclipper 使用 etcd 作为后端存储，为了保证高可用，建议使用 3 节点及以上来部署。 生产环境建议 server 节点和 agent 节点分离，避免某一主机同时作为 server 节点和 agent 节点。 主机要求：您仅需参考 准备工作 中对机器硬件和操作系统的要求准备多台主机。\n部署 KubeClipper 下载 kcctl KubeClipper 提供了命令行工具🔧 kcctl 以简化运维工作，您可以直接使用以下命令下载最新版 kcctl：\n# 默认安装最新发行版 curl -sfL https://oss.kubeclipper.io/kcctl.sh | bash - # 如果你在中国， 你可以在安装时使用 cn 环境变量, 此时我们会使用 registry.aliyuncs.com/google_containers 代替 k8s.gcr.io curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn bash - 您也可以在 GitHub Release Page 下载指定版本。\n通过以下命令检测是否安装成功:\nkcctl version 开始安装 您只需执行一个命令即可安装 KubeClipper，其模板如下所示：\nkcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) (--server SERVER_NODES) (--agent AGENT_NODES) 若使用 ssh passwd 方式则命令如下所示:\nkcctl deploy --user root --passwd $SSH_PASSWD --server SERVER_NODES --agent AGENT_NODES 私钥方式如下：\nkcctl deploy --user root --pk-file $SSH_PRIVATE_KEY --server SERVER_NODES --agent AGENT_NODES 您只需要提供 ssh user 以及 ssh passwd 或者 ssh 私钥即可在对应节点部署 KubeClipper。\n本教程使用 私钥 方式进行部署，具体命令如下：\nkcctl deploy --server 192.168.10.110,192.168.10.111,192.168.10.112 --agent 192.168.10.113,192.168.10.114,192.168.10.115 --pk-file ~/.ssh/id_rsa --pkg https://oss.kubeclipper.io/release/v1.1.0/kc-amd64.tar.gz 该命令指定 kubeclipper 包含 3 server 节点，3 agent 节点。\n您可以访问 GitHub Release Page 查看当前 KubeClipper 的 Release 版本，来修改 pkg 参数中的版本号。\n比如在 v1.2.0 版本 release 之后您可以指定 –pkg 为 https://oss.kubeclipper.io/release/v1.2.0/kc-amd64.tar.gz 来安装 v1.2.0 版本。\n执行该命令后，Kcctl 将检查您的安装环境，若满足条件将会进入安装流程。在打印出如下的 KubeClipper banner 后即表示安装完成。\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| 登录控制台 安装完成后，打开浏览器，访问 http://$IP 即可进入 KubeClipper 控制台。 您可以使用默认帐号密码 admin / Thinkbig1 进行登录。\n您可能需要配置端口转发规则并在安全组中开放端口，以便外部用户访问控制台。\n","categories":"","description":"通过简单的操作部署一个高可用的 kubeclipper\n","excerpt":"通过简单的操作部署一个高可用的 kubeclipper\n","ref":"/docs/deployment-docs/ha-env/","tags":"","title":"部署 HA"},{"body":"对于初次接触 KubeClipper 并想快速上手的用户，建议使用 All-in-One 安装模式，它能够帮助您零配置快速部署 KubeClipper。\n部署 KubeClipper 下载 kcctl KubeClipper 提供了命令行工具🔧 kcctl 以简化运维工作，您可以直接使用以下命令下载最新版 kcctl：\n# 默认安装最新的发行版 curl -sfL https://oss.kubeclipper.io/kcctl.sh | bash - # 安装指定版本 curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_VERSION=v1.2.1 bash - # 如果您在中国， 您可以在安装时使用 cn 环境变量, 此时我们会使用 registry.aliyuncs.com/google_containers 代替 k8s.gcr.io curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn bash - 您也可以在 GitHub Release Page 下载指定版本。\n通过以下命令检测是否安装成功:\nkcctl version 开始安装 在本快速入门教程中，您只需执行一个命令即可安装 KubeClipper，其模板如下所示：\nkcctl deploy 若使用 ssh passwd 方式则命令如下所示:\nkcctl deploy --user root --passwd $SSH_PASSWD 私钥方式如下：\nkcctl deploy --user root --pk-file $SSH_PRIVATE_KEY 您只需要提供 ssh user 以及 ssh passwd 或者 ssh 私钥即可在本机部署 KubeClipper。\n执行该命令后，Kcctl 将检查您的安装环境，若满足条件将会进入安装流程。在打印出如下的 KubeClipper banner 后即表示安装完成。\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| 您也可以部署 master 版本的 KubeClipper，来体验最新的功能特性\n安装 kcctl curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_VERSION=master bash - 在安装服务器上设置环境变量 export KC_VERSION=master 部署 KubeClipper AIO 环境 kcctl deploy 登录控制台 安装完成后，打开浏览器，访问 http://$IP 即可进入 KubeClipper 控制台。\n您可以使用默认帐号密码 admin / Thinkbig1 进行登录。\n您可能需要配置端口转发规则并在安全组中开放端口，以便外部用户访问控制台。\n创建 k8s 集群 部署成功后您可以使用 kcctl 工具或者通过控制台创建 k8s 集群。在本快速入门教程中使用 kcctl 工具进行创建。\n首先使用默认帐号密码进行登录获取 token，便于后续 kcctl 和 kc-server 进行交互。\nkcctl login -H http://localhost -u admin -p Thinkbig1 然后使用以下命令创建 k8s 集群:\nNODE=$(kcctl get node -o yaml|grep ipv4DefaultIP:|sed 's/ipv4DefaultIP: //') kcctl create cluster --master $NODE --name demo --untaint-master 大概 3 分钟左右即可完成集群创建,也可以使用以下命令查看集群状态\nkcctl get cluster -o yaml|grep status -A5 您也可以进入控制台查看实时日志。\n进入 Running 状态即表示集群安装完成,您可以使用 kubectl get cs 命令来查看集群健康状况。\n","categories":["QuickStart"],"description":"部署AIO环境\n","excerpt":"部署AIO环境\n","ref":"/docs/getting-started/aio-env/","tags":["aio","sample","docs"],"title":"部署AIO"},{"body":"","categories":"","description":"部署示例\n","excerpt":"部署示例\n","ref":"/docs/deployment-docs/","tags":"","title":"部署文档"},{"body":"在“节点信息”页面，您可以查看平台中管理的全部节点列表，节点规格、状态等信息。点击节点名称进入节点详情页面，您可以查看详细的节点基本信息和系统信息。\nKubeClipper 中的节点状态表示 kc-gent 对节点的管理状态。正常情况下，节点状态显示为“就绪”，当节点失联4分钟（误差时间10s内）后，状态会更新为“未知”，未知状态的节点无法进行任何操作，也无法创建集群或为集群添加/移除节点。\n添加节点 在部署 KubeClipper 时，您就可以添加初始的 server 节点和 agent 节点，其中，server节点用于部署 KubeClipper 自身服务，agent 节点可用于部署 K8S 集群。在用于实验或开发的 KubeClipper 环境，您可以将 server 节点同时添加为 agent 节点。但如果用于生产环境，建议不要将 server 节点复用为 agent 节点。\n您也可以使用 kcctl join 命令为 KubeClipper 添加 agent 节点。同时，您可以为每个 agent 节点标记一个区域，区域可以是物理的或逻辑的位置，您可以使用同一区域的节点创建 K8S 集群，但不可以使用跨区域的节点创建集群。未标记区域的节点默认属于 default 区域。详情参见“Kcctl 操作指南“。\n命令行示例：\nkcctl join --agent beijing:1.2.3.4 --agent shanghai:2.3.4.5 移除节点 当您不再需要某些节点，可以使用 kcctl drain 命令将节点从平台中移除。详情参见“Kcctl 操作指南“。\n命令行示例：\nkcctl drain --agent 192.168.10.19 连接终端 在节点列表页面，您可以点击目标节点右侧的“连接终端”按钮，在连接终端的弹窗中输入节点端口和用户名密码信息后，访问节点SSH控制台并执行命令。\n","categories":"","description":"KubeClipper 节点管理功能使用指南\n","excerpt":"KubeClipper 节点管理功能使用指南\n","ref":"/docs/tutorials/node-manage/","tags":"","title":"节点管理"},{"body":"1. Go to the creation screen Log in to the Kubeclipper platform and click the button as shown in the figure to enter the cluster creation interface\n2. Configure cluster nodes Follow the text prompts to complete the steps of entering the cluster name and selecting nodes\nNote: The number of master nodes cannot be an even number.\n3. Configure cluster This step is used to configure the cluster network and components such as the database and container runtime\nSelect offline installation and fill in the address of the image repository you have built first\n4. Configure cluster storage Select nfs storage and follow the text prompts to fill in the appropriate fields\n5. Installation completed Complete all configurations to confirm installation\nInstallation is successful and the cluster is up and running\n","categories":"","description":"How to create a k8s cluster offline using the KC platform\n","excerpt":"How to create a k8s cluster offline using the KC platform\n","ref":"/en/docs/getting-started/carete-k8s-cluster-offline/","tags":"","title":"Create k8s clusters offline using the kubeclipper platform"},{"body":"Create user After installing KubeClipper, you need to create a user for the desired role. Initially, the system has only one user, admin, by default, with the platform administrator role.\nClick \"Access Control\" \u003e \"Users\" to enter the user management page, click the \"Create User\" button in the upper left corner, fill in the user name, password, mobile phone number, email and other information in the pop-up window, specify the user role, and click the \"OK\" button. The four built-in roles in the system are as follows:\nPlatform administrator: have platform configuration, cluster management, user management and other platform viewing and operation rights.\nCluster Administrator: Have all cluster management rights.\nUser Administrator: Have all user management rights.\nPlatform read-only users: have platform viewing rights.\nAfter the user is created, you can view the user details and login logs on the user details page and do the following:\nEdit: Edit user alias, role, mobile phone number, email information.\nEdit Password: Edit the user login password.\nDelete: Delete the user.\nCreate a custom role In addition to system built-in roles, you can also create custom roles to meet business needs.\nClick \"Access Control\" \u003e \"Roles\" to enter the role management page. You can click the \"Create Role\" button in the upper left corner to create a custom role.\nOn the Create Role page, you need to fill in the role name and description, and check the permissions required to customize the role. Some permissions depend on other permissions. When these permissions are selected, the dependent permissions will be automatically selected.\nAfter creating a custom role, you can view the basic role information, role permission list, authorized user list on the role details page, and perform the following operations for the custom role:\nEdit: Edit the custom role alias.\nEdit permissions: Edit permissions under the custom role.\nDelete: To delete a custom role, make sure that no user is using the role to be deleted.\nAccess to external users KubeClipper can log in using external users via the OIDC protocol .\nFirst, the platform administrator needs to log in to the platform server node and insert the following information under authentication in the kubeclipepr-server.yaml file:\noauthOptions: identityProviders: - name: keycloak type: OIDC mappingMethod: auto provider: clientID: kc clientSecret: EErn729BB1bKawdRtnZgrqj9Bx0]mzUs issuer: http://172.20.163.233:7777/auth/realms/kubeclipper scopes: - openid - email redirectURL: http://{kc-console-address}/oatuh2/redirect/{IDP-Name} Under \"provider\", you need to fill in the clientID , clientSecret , and issuer information of your OAuth2 service, taking keycloack as an example, as shown in the figure below.\nRedirectURL example: http://172.0.0.90/oauth2/redirect/keycloack\nOAuth2 users can access and use the KubeClipper platform by following these steps:\nClick the \"OAuth2 Login\" button on the login page, enter the OAuth2 login page, enter the username and password to log in, enter the KubeClipper platform. When logging in for the first time, you will not be able to access the platform because you have not been granted any permission.\nThe platform administrator or other user with user management rights log in to KubeClipper, find the target OAuth2 user on the user management page, and set the user role by editing the user information.\nThe OAuth2 user repeats the first step, logs in to KubeClipper, and can access the platform normally and perform operations within the role permissions.\n","categories":"","description":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. It can be **bold** or _italic_ and can be split over multiple paragraphs.\n","ref":"/en/docs/tutorials/access-control/","tags":"","title":"Access control"},{"body":"","categories":"","description":"Deploying the sample\n","excerpt":"Deploying the sample\n","ref":"/en/docs/deployment-docs/","tags":"","title":"Deployment docs"},{"body":"创建用户 安装 KubeClipper 之后，您需要创建所需角色的用户。一开始，系统默认只有一个用户 admin，具有平台管理员角色。\n点击“访问控制”\u003e“用户”，进入用户管理页面，点击左上角“创建用户”按钮，在弹窗中填写用户名、密码、手机号码、邮箱等信息，并指定用户角色，点击“确认”按钮。系统内置四个角色如下：\n平台管理员：拥有平台配置、集群管理、用户管理、DNS解析、审计等全部平台查看和操作权限。\n集群管理员：拥有所有集群管理权限。\n用户管理员：拥有所有用户管理权限。\n平台只读用户：拥有全部平台查看权限。\n用户创建完成后，您可以在用户详情页面查看用户详情信息和登录日志，并执行以下操作：\n编辑：编辑用户别名、角色、手机号、邮箱信息。\n编辑密码：编辑用户登录密码。\n删除：删除用户。\n创建自定义角色 除了系统内置角色，您也可以创建自定义角色，已满足业务需要。\n点击“访问控制”\u003e“角色”，进入角色管理页面，您可以点击左上角“创建角色”按钮，创建自定义角色。\n在创建角色页面，您需要填写角色名称和描述，并勾选自定义角色所需权限，一些权限依赖于其他权限，在选择这些权限时，将自动选中依赖的权限。\n创建自定义角色完成后，您可以在角色详情页面查看角色基本信息、角色权限列表、授权用户列表，并对自定义角色执行以下操作：\n编辑：编辑自定义角色别名。\n编辑权限：编辑自定义角色下的权限。\n删除：删除自定义角色，需确保没有用户使用待删除角色。\n接入外部用户 KubeClipper 可以通过 OIDC 协议使用外部用户登录。\n首先，平台管理员需要登录平台 server 节点，在 kubeclipepr-server.yaml 文件中的 authentication 下插入以下信息：\noauthOptions: identityProviders: - name: keycloak type: OIDC mappingMethod: auto provider: clientID: kc clientSecret: EErn729BB1bKawdRtnZgrqj9Bx0]mzUs issuer: http://172.20.163.233:7777/auth/realms/kubeclipper scopes: - openid - email redirectURL: http://{kc-console-address}/oatuh2/redirect/{IDP-Name} 其中，“provider”下需要您填写自己的 OAuth2 服务的clientID、clientSecret、issuer信息，以 keycloack 为例，如下图所示。\nredirectURL示例：http://172.0.0.90/oauth2/redirect/keycloack\nOAuth2 用户可以通过以下步骤访问和使用 KubeClipper 平台：\n点击登录页的“OAuth2 登录”按钮，进入 OAuth2 登录页面，输入用户名密码登录，进入 KubeClipper 平台，首次登录，您会因未被授予权限而无法访问平台。\n平台管理员或其他拥有用户管理权限的用户登录 KubeClipper，在用户管理页面，找到目标 OAuth2 用户，通过编辑用户指定用户角色。\nOAuth2 用户重复第一步，登录 KubeClipper，就可以正常访问平台并执行角色权限内操作。\n","categories":"","description":"KubeClipper 访问控制功能使用指南\n","excerpt":"KubeClipper 访问控制功能使用指南\n","ref":"/docs/tutorials/access-control/","tags":"","title":"访问控制"},{"body":"","categories":"","description":"用户使用手册\n","excerpt":"用户使用手册\n","ref":"/docs/tutorials/","tags":"","title":"使用手册"},{"body":"1. 进入创建界面 登录 Kubeclipper 平台后点击如图所示按钮，进入集群创建界面\n2. 配置集群节点 按照文字提示完成输入集群名称、选择节点等步骤\n注意: master 节点数量不能为偶数\n3. 配置集群 此步骤用于配置集群网络以及数据库、容器运行时等组件\n选择离线安装并填写首先搭建好的镜像仓库地址\n4. 配置存储 选择 nfs 存储，按照文字提示填写相应内容\n5. 安装完成 完成所有配置确认安装\n安装成功，集群正常运行\n","categories":"","description":"如何使用 KC 平台离线创建 k8s 集群\n","excerpt":"如何使用 KC 平台离线创建 k8s 集群\n","ref":"/docs/getting-started/carete-k8s-cluster-offline/","tags":"","title":"使用 Kubeclipper 离线创建 k8s 集群"},{"body":"","categories":"","description":"常见问题记录\n","excerpt":"常见问题记录\n","ref":"/docs/faq/","tags":"","title":"常见问题"},{"body":"问题复现 安装 v1.2.1 版本的 kcctl\ncurl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_VERSION=v1.2.1 bash - 通过 kcctl deploy 命令部署 KubeClipper 集群\n# 安装 AIO 环境 kcctl deploy 通过 kcctl create cluster 命令创建 k8s 集群\n# 需要先登录 kcctl login --host http://127.0.0.1 --username admin --password Thinkbig1 # 创建集群 kcctl create cluster --name test --master 192.168.10.98 --untaint-master 登录 KubeClipper 管理界面，查看创建集群操作日志，日志显示在安装 cni 过程中发现下载 calico v3.21.2 404 无法找到\n问题修复 PR 提交已经合并到了master，PR：https://github.com/kubeclipper/kubeclipper/commit/7e6eb0ed199ff1cb00fde0c2624c62cdc5ca0b9c\n但 v1.2.1 已经发布了，按照发版规范无法在该版本打补丁，需要等到后续 v1.2.2 发布解决，因此我们提供一种临时方案来解决这个问题。\n解决方法 制作离线资源包 下载 calico v3.21.2 的安装包，打包为指定格式的离线资源包\n# 创建资源目录 mkdir -pv calico/v3.21.2/amd64 # 下载 v3.21.2 版本的 calico wget -P calico/v3.21.2/amd64 https://oss.kubeclipper.io/packages/calico/v3.21.2/amd64/images.tar.gz wget -P calico/v3.21.2/amd64 https://oss.kubeclipper.io/packages/calico/v3.21.2/amd64/manifest.json # 压缩文件为指定命令 tar -zcvf calico-v3.21.2-amd64.tar.gz calico 推送离线资源包\n# 推送 kcctl resource push --pkg calico-v3.21.2-amd64.tar.gz --type cni # 验证 kcctl resource list|grep v3.21.2 如果在执行 kcctl resource push 报了如下错误： 解决方法如下：\n编辑 /root/.kc/deploy-config.yaml 文件。 找到 ssh 配置项，添加 pkFile 字段配置，值为当前服务器的 ssh 公钥文件的绝对路径。\n通过命令行安装 k8s 集群，在 KubeClipper 管理后台查看操作日志\nkcctl create cluster --name test --master 192.168.10.98 --untaint-master 查看 k8s 集群 pods 运行状态\nkubectl get pods -A ","categories":["FAQ"],"description":"在 v1.2.1 版本（包括v1.2.1）之前，使用 `kcctl create cluster` 命令创建集群功能，会发生创建失败错误，以下提供一种临时的解决方法。 在 v1.2.1 版本之后，我们修复了该问题。\n","excerpt":"在 v1.2.1 版本（包括v1.2.1）之前，使用 `kcctl create cluster` 命令创建集群功能，会发生创建失败错误，以下提供一种临时的解决方法。 在 v1.2.1 版本之后，我们修复了该问题。\n","ref":"/docs/faq/kcctl-create-cluster-error/","tags":["kcctl","create","cluster"],"title":"通过kcctl命令创建集群错误"},{"body":"","categories":"","description":"User manual\n","excerpt":"User manual\n","ref":"/en/docs/tutorials/","tags":"","title":"Tutorials"},{"body":"请查看 KubeClipper Community\n","categories":"","description":"如何为kubeclipper做贡献\n","excerpt":"如何为kubeclipper做贡献\n","ref":"/docs/contribution-guidelines/","tags":"","title":"贡献指南"},{"body":"Please check KubeClipper Community\n","categories":"","description":"How to contribute to the docs\n","excerpt":"How to contribute to the docs\n","ref":"/en/docs/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/blog/news/","tags":"","title":"News About Docsy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/blog/releases/","tags":"","title":"New Releases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cluster/","tags":"","title":"cluster"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/create/","tags":"","title":"create"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/faq/","tags":"","title":"FAQ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kcctl/","tags":"","title":"kcctl"},{"body":" 欢迎来到 Kubeclipper ! 了解更多 免费下载 用最简单的方式管理 kubenertes !\nKubeClipper 是一个轻量级、易用的图形界面 Kubernetes 集群生命周期管理平台。\n它与 K8S 完全兼容，用户可以通过友好的向导式 Web UI，在客户管理的基础架构上快速部署企业所需的 K8S 集群，并提供持续的全生命周期管理能力。\n易使用 友好的向导式图形化界面，初学者也可以快速部署一个生产级集群并安装所需插件。\n极轻量 架构简单，少依赖，平台部署仅需两个命令行。\n生产级 易用性和专业性兼顾，支持丰富的集群参数配置和插件管理，满足生产级集群部署需求。\n立即体验 1. 下载 kcctl 2. 部署 KubeClipper curl -sfL https://oss.kubeclipper.io/kcctl.sh | sh - # In China, you can add cn env, we use registry.aliyuncs.com/google_containers instead of k8s.gcr.io curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn sh - kcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) ","categories":"","description":"","excerpt":" 欢迎来到 Kubeclipper ! 了解更多 免费下载 用最简单的方式管理 kubenertes !\nKubeClipper 是一个轻量级、易用的图形界面 Kubernetes 集群生命周期管理平台。\n它与 K8S 完全兼容，用户可以通过友好的向导式 Web UI，在客户管理的基础架构上快速部署企业所需的 K8S 集群，并提供持续的全生命周期管理能力。\n易使用 友好的向导式图形化界面，初学者也 …","ref":"/","tags":"","title":"Kubeclipper"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/tags/aio/","tags":"","title":"aio"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/aio/","tags":"","title":"aio"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docs/","tags":"","title":"docs"},{"body":" Welcome to Kubeclipper! Learn More Download Manage Kubernetes in the most light and simple way !\nKubeClipper is a lightweight, simple graphical interface Kubernetes cluster lifecycle management platform.\nIt is fully compatible with K8S, allows users to quickly deploy the K8S clusters required by the enterprise on customer-managed infrastructures through a friendly wizard-like Web UI, and provides continuous full life cycle management capabilities.\nSimple Provides friendly wizard graphical interface, allows beginners to quickly deploy a cluster and the required plug-ins.\nLightweight Simple architecture, few dependencies, only two command lines for platform deployment.\nProfessional Simple and professional, it supports rich cluster parameter configuration and plug-in management, meeting production-level cluster deployment requirements.\nQuick Start 1. Download kcctl 2. Deploy KubeClipper curl -sfL https://oss.kubeclipper.io/kcctl.sh | sh - # In China, you can add cn env, we use registry.aliyuncs.com/google_containers instead of k8s.gcr.io curl -sfL https://oss.kubeclipper.io/kcctl.sh | KC_REGION=cn sh - kcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) ","categories":"","description":"","excerpt":" Welcome to Kubeclipper! Learn More Download Manage Kubernetes in the most light and simple way !\nKubeClipper is a lightweight, simple graphical interface Kubernetes cluster lifecycle management …","ref":"/en/","tags":"","title":"Kubeclipper"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/categories/quickstart/","tags":"","title":"QuickStart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/quickstart/","tags":"","title":"QuickStart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/tags/sample/","tags":"","title":"sample"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sample/","tags":"","title":"sample"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/tags/","tags":"","title":"Tags"},{"body":"This is a typical blog post that includes images.\nThe front matter specifies the date of the blog post, its title, a short description that will be displayed on the blog landing page, and its author.\nIncluding images Here’s an image (featured-sunset-get.png) that includes a byline and a caption.\nFetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\nThe front matter of this post specifies properties to be assigned to all image resources:\nresources: - src: \"**.{png,jpg}\" title: \"Image #:counter\" params: byline: \"Photo: Riona MacNamara / CC-BY-CA\" To include the image in a page, specify its details like this:\nFetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\nThe image will be rendered at the size and byline specified in the front matter.\n","categories":"","description":"The Docsy Hugo theme lets project maintainers and contributors focus on content, not on reinventing a website infrastructure from scratch","excerpt":"The Docsy Hugo theme lets project maintainers and contributors focus on content, not on reinventing a website infrastructure from scratch","ref":"/en/blog/2018/10/06/easy-documentation-with-docsy/","tags":"","title":"Easy documentation with Docsy"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","ref":"/en/blog/","tags":"","title":"Docsy Blog"},{"body":"Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\nFirst Header This is a normal paragraph following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nBacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nLorem markdownum tuta hospes stabat; idem saxum facit quaterque repetito occumbere, oves novem gestit haerebat frena; qui. Respicit recurvam erat: pignora hinc reppulit nos aut, aptos, ipsa.\nMeae optatos passa est Epiros utiliter Talibus niveis, hoc lata, edidit. Dixi ad aestum.\nHeader 2 This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nHeader 3 This is a code block following a header. Header 4 This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header. Header 5 This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header. Header 6 What Follows A table A header A table A header A table A header There’s a horizontal rule above and below this.\nHere is an unordered list:\nSalt-n-Pepa Bel Biv DeVoe Kid ‘N Play And an ordered list:\nMichael Jackson Michael Bolton Michael Bublé And an unordered task list:\nCreate a sample markdown document Add task lists to it Take a vacation And a “mixed” task list:\nSteal underpants ? Profit! And a nested list:\nJackson 5 Michael Tito Jackie Marlon Jermaine TMNT Leonardo Michelangelo Donatello Raphael Definition lists can be used with Markdown syntax. Definition terms are bold.\nName Godzilla Born 1952 Birthplace Japan Color Green Tables should have bold headings and alternating shaded rows.\nArtist Album Year Michael Jackson Thriller 1982 Prince Purple Rain 1984 Beastie Boys License to Ill 1986 If a table is too wide, it should scroll horizontally.\nArtist Album Year Label Awards Songs Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin’ Somethin’, Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\"; Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) } Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this. Inline code inside table cells should still be distinguishable.\nLanguage Code Javascript var foo = \"bar\"; Ruby foo = \"bar\"{ Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nComponents Alerts This is an alert. Note: This is an alert with a title. This is a successful alert. This is a warning! Warning! This is a warning with a title! Sizing Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nParameters available Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing pixels Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing rem Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMemory Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nRAM to use Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMore is better Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsed RAM Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis is the final element on the page and there should be no margin below this. ","categories":"","description":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","ref":"/en/blog/2018/10/06/the-second-blog-post/","tags":"","title":"The second blog post"},{"body":"Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\nFirst Header This is a normal paragraph following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nBacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nLorem markdownum tuta hospes stabat; idem saxum facit quaterque repetito occumbere, oves novem gestit haerebat frena; qui. Respicit recurvam erat: pignora hinc reppulit nos aut, aptos, ipsa.\nMeae optatos passa est Epiros utiliter Talibus niveis, hoc lata, edidit. Dixi ad aestum.\nHeader 2 This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nHeader 3 This is a code block following a header. Header 4 This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header. Header 5 This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header. Header 6 What Follows A table A header A table A header A table A header There’s a horizontal rule above and below this.\nHere is an unordered list:\nSalt-n-Pepa Bel Biv DeVoe Kid ‘N Play And an ordered list:\nMichael Jackson Michael Bolton Michael Bublé And an unordered task list:\nCreate a sample markdown document Add task lists to it Take a vacation And a “mixed” task list:\nSteal underpants ? Profit! And a nested list:\nJackson 5 Michael Tito Jackie Marlon Jermaine TMNT Leonardo Michelangelo Donatello Raphael Definition lists can be used with Markdown syntax. Definition terms are bold.\nName Godzilla Born 1952 Birthplace Japan Color Green Tables should have bold headings and alternating shaded rows.\nArtist Album Year Michael Jackson Thriller 1982 Prince Purple Rain 1984 Beastie Boys License to Ill 1986 If a table is too wide, it should scroll horizontally.\nArtist Album Year Label Awards Songs Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin’ Somethin’, Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\"; Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) } Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this. Inline code inside table cells should still be distinguishable.\nLanguage Code Javascript var foo = \"bar\"; Ruby foo = \"bar\"{ Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nComponents Alerts This is an alert. Note: This is an alert with a title. This is a successful alert. This is a warning! Warning! This is a warning with a title! Sizing Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nParameters available Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing pixels Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing rem Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMemory Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nRAM to use Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMore is better Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsed RAM Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis is the final element on the page and there should be no margin below this. ","categories":"","description":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","ref":"/en/blog/2018/01/04/another-great-release/","tags":"","title":"Another Great Release"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/en/community/","tags":"","title":"Community"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/categories/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/categories/placeholders/","tags":"","title":"Placeholders"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/en/tags/test/","tags":"","title":"test"},{"body":"Comming soon…\n","categories":"","description":"","excerpt":"Comming soon…\n","ref":"/blog/","tags":"","title":"KubeClipper Blog"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/community/","tags":"","title":"社区"}]